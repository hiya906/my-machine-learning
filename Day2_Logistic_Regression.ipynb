{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day2. Logistic Regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiya906/my-machine-learning/blob/master/Day2_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyqq7DFfuRXd",
        "colab_type": "text"
      },
      "source": [
        "# **사용법**\n",
        "\n",
        "1.   우측 상단 '로그인'\n",
        "2.   좌측 상단 '실습 모드에서 열기'\n",
        "\n",
        "\n",
        "※ 각각의 셀은 셀 좌측 상단 실행 버튼을 통해 실행할 수 있습니다.\n",
        "\n",
        "※ 실행 중 '경고: 이 노트는 Google에서 작성하지 않았습니다.'라는 창이 뜰 경우, '실행 전에 모든 런타임 재설정'란에 체크 후 '무시하고 계속하기'를 하시면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIV8iSKBhz5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터를 관리하기 위해서 폴더를 생성하기 위한 코드입니다\n",
        "import os\n",
        "\n",
        "os.mkdir('./data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV6rMy-du32V",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression by Numpy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5rZ-h4mxTNV",
        "colab_type": "text"
      },
      "source": [
        "# 실습1: Logistic Regression with Titanic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iiU9W3axqMn",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title 필요 Package Import \n",
        "\n",
        "import os                            # 데이터 파일 경로 설정\n",
        "import csv                           # 데이터 파일 로드\n",
        "import numpy as np                   # numpy 행렬 조작\n",
        "import matplotlib.pyplot as plt      # 그래프 그리기(선택 사항)\n",
        "\n",
        "np.random.seed(123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K30sqQtnzq4g",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title 데이터 준비\n",
        "\n",
        "def Titanic_Dataset(path, filename):\n",
        "    file = os.path.join(path, filename)\n",
        "\n",
        "    with open(file, 'r') as f:\n",
        "        csv_reader = csv.reader(f)                  # 파일 로드\n",
        "        header = next(csv_reader)\n",
        "\n",
        "        x_data = []\n",
        "        y_data = []\n",
        "        for line in csv_reader:\n",
        "            features = line[1:]\n",
        "            x = [1] + list(map(float, features))    # x_data에 bias를 위한 1추가\n",
        "            y = float(line[0])\n",
        "\n",
        "            x_data.append(x)\n",
        "            y_data.append(y)\n",
        "\n",
        "        x_array = np.array(x_data)\n",
        "        y_array = np.array(y_data)\n",
        "\n",
        "    return header, x_array, y_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opVsUXxj3S7o",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression Class 선언\n",
        "\n",
        "1.   __init__\n",
        "\n",
        "\n",
        "> *   인자: 모델 설정 \n",
        "*   출력: x\n",
        "*   기능: 모델 초기화\n",
        "\n",
        "> weight *W*를 random하게 initialization\n",
        "\n",
        "\n",
        "2.   __train__\n",
        "\n",
        "\n",
        "> *   입력: 학습데이터, 학습 설정\n",
        "*   출력: Loss \n",
        "*   기능: 데이터로 모델 학습\n",
        "\n",
        "> 매 epoch마다 전체 데이터에 대해 loss, grad 계산하여 학습\n",
        "\n",
        "\n",
        "3. __eval__\n",
        "\n",
        "> *   입력: 검증 데이터\n",
        "*   출력: 모델의 예측값\n",
        "*   기능: train로 학습된 모델로 검증, 예측값 생성\n",
        "\n",
        "> 검증 데이터에 대해 분류 예측 결과 산출 \n",
        "\n",
        "4. ___sigmoid__\n",
        "\n",
        "> *   입력: 실수형 numpy array\n",
        "*   출력: sigmoid를 취한 array\n",
        "*   기능: 주어진 array에 대한 모든 sigmoid 값 계산\n",
        "\n",
        "> $sigmoid(x) =\\frac{1}{ 1+e^{-(x)}}$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zABdKBI5NGh",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title 정답 코드\n",
        "class LogisticRegression:\n",
        "  def __init__(self, num_features):\n",
        "      self.W = np.random.rand(num_features, 1) * 0.01\n",
        "\n",
        "  def train(self, train_x, train_y, num_epochs, learning_rate):\n",
        "\n",
        "      epsilon = 1e-10\n",
        "      loss_memory = []\n",
        "      train_y = np.expand_dims(train_y, 1)\n",
        "\n",
        "      for epoch in range(num_epochs):\n",
        "          # ====================== 아래에 코드를 작성해주세요 ======================\n",
        "          # 1-1. logit 계산\n",
        "          logit = np.matmul(train_x, self.W)\n",
        "\n",
        "          # 1-2. sigmoid 적용\n",
        "          prob = self._sigmoid(logit)\n",
        "\n",
        "          # 1-3. Error 및 Loss 계산\n",
        "          error = prob - train_y\n",
        "\n",
        "          loss = train_y * np.log(prob + epsilon) + (1 - train_y) * np.log(1 - prob + epsilon)\n",
        "          loss = - np.sum(loss)\n",
        "\n",
        "\n",
        "          # print(loss)\n",
        "          # 1-4. Loss ‘loss_memory’에 추가\n",
        "          loss_memory.append(loss)\n",
        "\n",
        "          # 1-5. Gradient 계산\n",
        "          grad = np.mean(train_x * error, axis=0, keepdims=True).T\n",
        "\n",
        "          # 1-6. Weight Update\n",
        "          self.W -= grad * learning_rate\n",
        "          # ==================================================================\n",
        "\n",
        "      # 2. ‘loss_memory’ 반환\n",
        "      return loss_memory\n",
        "\n",
        "\n",
        "  def predict(self, test_x):\n",
        "      prob = self._sigmoid(np.matmul(test_x, self.W))\n",
        "\n",
        "      return prob\n",
        "\n",
        "  def _sigmoid(self, x):\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "      \n",
        "def Accuracy(prob, true):\n",
        "  Acc = 0\n",
        "  \n",
        "  #Acc 계산\n",
        "  pred = np.round(prob, 0)\n",
        "  correct = np.sum(pred.squeeze() == true.squeeze())\n",
        "  num_data = pred.shape[0]\n",
        "\n",
        "  Acc = correct / num_data\n",
        "\n",
        "  return Acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGjGL5rb6yW3",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title 모델 학습\n",
        "num_epochs = 1000\n",
        "learning_rate = 1e-3\n",
        "\n",
        "header, train_x, train_y = Titanic_Dataset('./data', 'Titanic_train.csv')\n",
        "num_data, num_features = train_x.shape\n",
        "\n",
        "model = LogisticRegression(num_features)\n",
        "loss_memory = model.train(train_x, train_y, num_epochs, learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxvkpeIl7ASw",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6f6416d9-9c16-4352-cf2f-13ed2e71f4c5"
      },
      "source": [
        "#@title 모델 평가\n",
        "\n",
        "_, test_x, test_y = Titanic_Dataset('./data', 'Titanic_test.csv')\n",
        "pred = model.predict(test_x)\n",
        "acc = Accuracy(pred, test_y)\n",
        "\n",
        "print('Accuracy: ', acc)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7037037037037037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMOeMD-JoBqr",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title 정답 코드\n",
        "class LogisticRegression:\n",
        "  def __init__(self, num_features):\n",
        "      self.W = np.random.rand(num_features, 1) * 0.01\n",
        "\n",
        "  def train(self, train_x, train_y, num_epochs, learning_rate):\n",
        "\n",
        "      epsilon = 1e-10\n",
        "      loss_memory = []\n",
        "      train_y = np.expand_dims(train_y, 1)\n",
        "\n",
        "      for epoch in range(num_epochs):\n",
        "          # ====================== 아래에 코드를 작성해주세요 ======================\n",
        "          # 1-1. logit 계산\n",
        "          logit = np.matmul(train_x, self.W)\n",
        "\n",
        "          # 1-2. sigmoid 적용\n",
        "          prob = self._sigmoid(logit)\n",
        "\n",
        "          # 1-3. Error 및 Loss 계산\n",
        "          error = prob - train_y\n",
        "\n",
        "          loss = train_y * np.log(prob + epsilon) + (1 - train_y) * np.log(1 - prob + epsilon)\n",
        "          loss = - np.sum(loss)\n",
        "\n",
        "\n",
        "          # print(loss)\n",
        "          # 1-4. Loss ‘loss_memory’에 추가\n",
        "          loss_memory.append(loss)\n",
        "\n",
        "          # 1-5. Gradient 계산\n",
        "          grad = np.mean(train_x * error, axis=0, keepdims=True).T\n",
        "\n",
        "          # 1-6. Weight Update\n",
        "          self.W -= grad * learning_rate\n",
        "          # ==================================================================\n",
        "\n",
        "      # 2. ‘loss_memory’ 반환\n",
        "      return loss_memory\n",
        "\n",
        "\n",
        "  def predict(self, test_x):\n",
        "      prob = self._sigmoid(np.matmul(test_x, self.W))\n",
        "\n",
        "      return prob\n",
        "\n",
        "  def _sigmoid(self, x):\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "      \n",
        "def Accuracy(prob, true):\n",
        "  Acc = 0\n",
        "  \n",
        "  #Acc 계산\n",
        "  pred = np.round(prob, 0)\n",
        "  correct = np.sum(pred.squeeze() == true.squeeze())\n",
        "  num_data = pred.shape[0]\n",
        "\n",
        "  Acc = correct / num_data\n",
        "\n",
        "  return Acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb1xNE580xLB",
        "colab_type": "text"
      },
      "source": [
        "# 실습2: Logistic Regression with Iris data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHBXvY091wul",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title 필요 Package import\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVpucGNc3VOS",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title 데이터 준비\n",
        "def Iris_Dataset(path, filename):\n",
        "    file = os.path.join(path, filename)\n",
        "\n",
        "    with open(file, 'r') as f:\n",
        "        csv_reader = csv.reader(f)\n",
        "        header = next(csv_reader)\n",
        "\n",
        "        x_data = []\n",
        "        y_data = []\n",
        "        for line in csv_reader:\n",
        "\n",
        "            features = line[:-1]\n",
        "            x = [1] + list(map(float, features))\n",
        "            y = float(line[-1])\n",
        "\n",
        "            x_data.append(x)\n",
        "            y_data.append(y)\n",
        "\n",
        "        x_array = np.array(x_data)\n",
        "        y_array = np.array(y_data)\n",
        "\n",
        "        return header, x_array, y_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1CRIAFF3j-_",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title 모델 학습\n",
        "num_epochs = 10000\n",
        "learning_rate = 1e-2\n",
        "\n",
        "header, train_x, train_y = Iris_Dataset('./data', 'Iris_Train.csv')\n",
        "\n",
        "num_data, num_features = train_x.shape\n",
        "model = LogisticRegression(num_features)\n",
        "\n",
        "loss_memory = model.train(train_x, train_y, num_epochs, learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHua6mI45Wu9",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ca557643-63fc-4092-e5ee-4f0844028afc"
      },
      "source": [
        "#@title 모델 평가\n",
        "_, test_x, test_y = Iris_Dataset('./data', 'Iris_Test.csv')\n",
        "\n",
        "pred = model.predict(test_x)\n",
        "accuracy = Accuracy(pred, test_y)\n",
        "\n",
        "print('Accuracy : ', accuracy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy :  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PII_E5Nx5cdm",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "fefb5827-4cdd-41e1-e54c-3ffe549dc007"
      },
      "source": [
        "#@title 그래프 표현\n",
        "is_virginica_x = test_x[test_y == 1]\n",
        "is_virginica_y = test_y[test_y == 1]\n",
        "\n",
        "not_virginica_x = test_x[test_y == 0]\n",
        "not_virginica_y = test_y[test_y == 0]\n",
        "\n",
        "\n",
        "plot_feat = np.linspace(0, 3, 100).reshape(-1, 1)\n",
        "plot_bias = np.ones_like(plot_feat)\n",
        "\n",
        "plot_x = np.concatenate((plot_bias, plot_feat), axis=1)\n",
        "plot_prob = model.predict(plot_x)\n",
        "\n",
        "plt.scatter(is_virginica_x[:, 1], is_virginica_y, color='b', label='Iris-Virginica')\n",
        "plt.scatter(not_virginica_x[:, 1], not_virginica_y, color='r', label='Not Iris-Verginica')\n",
        "plt.plot(plot_feat, plot_prob, 'g-', label='Hypothesis')\n",
        "\n",
        "plt.xlabel(\"Petal width\", fontsize=14)\n",
        "plt.ylabel(\"Probability\", fontsize=14)\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEOCAYAAAB8aOvdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQjk7ZaH6epP",
        "colab_type": "text"
      },
      "source": [
        "# 실습3: Logistic Regression with Tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBmq-AMF8NGg",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title 필요 Package import\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_random_seed(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-E6DYil93LY",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title 데이터 준비\n",
        "\n",
        "num_epochs = 2000\n",
        "learning_rate = 1e-3\n",
        "\n",
        "header, train_x, train_y = Titanic_Dataset('./data', 'Titanic_train.csv')\n",
        "_, test_x, test_y = Titanic_Dataset('./data', 'Titanic_test.csv')\n",
        "\n",
        "num_data, num_features = train_x.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL9NtcPl-NPh",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title 모델 생성\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None, num_features])\n",
        "Y = tf.placeholder(tf.float32, shape=[None])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([num_features, 1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "# model prediction \n",
        "logit = tf.matmul(X, W) + b\n",
        "prob = tf.sigmoid(logit)\n",
        "\n",
        "# Loss 및 학습법 정의\n",
        "epsilon = 1e-10\n",
        "loss = -tf.reduce_mean(Y * tf.log(prob + epsilon) + (1 - Y)*tf.log((1-prob) + epsilon))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate= learning_rate).minimize(loss)\n",
        "\n",
        "#평가 방법\n",
        "pred = tf.cast(prob > 0.5, dtype=tf.float32)\n",
        "correct = tf.cast(tf.equal(pred, Y), dtype=tf.float32)\n",
        "Accuracy = tf.reduce_mean(correct)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEgaucyc-34g",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "7914adcd-1b77-4822-953d-257946d597f1"
      },
      "source": [
        "#@title 모델 학습\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(1, num_epochs+1):\n",
        "    loss_val, training = sess.run([loss, train], feed_dict={X: train_x, Y: train_y})\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print('step: ', step, 'loss: ', loss_val)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step:  100 loss:  4.751186\n",
            "step:  200 loss:  1.8603483\n",
            "step:  300 loss:  0.9127513\n",
            "step:  400 loss:  0.9016853\n",
            "step:  500 loss:  0.8912981\n",
            "step:  600 loss:  0.88155425\n",
            "step:  700 loss:  0.87241703\n",
            "step:  800 loss:  0.86385393\n",
            "step:  900 loss:  0.8558255\n",
            "step:  1000 loss:  0.8483006\n",
            "step:  1100 loss:  0.841246\n",
            "step:  1200 loss:  0.8346318\n",
            "step:  1300 loss:  0.82842565\n",
            "step:  1400 loss:  0.82260317\n",
            "step:  1500 loss:  0.8171359\n",
            "step:  1600 loss:  0.81199867\n",
            "step:  1700 loss:  0.80716854\n",
            "step:  1800 loss:  0.80262357\n",
            "step:  1900 loss:  0.79834276\n",
            "step:  2000 loss:  0.7943066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3soL8pMo-pPA",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "fd50432e-ebdb-4db1-e942-1b3fcf5d37ca"
      },
      "source": [
        "#@title 모델 평가\n",
        "_prob, _pred, _accuracy = sess.run([prob, pred, Accuracy], feed_dict={X: test_x, Y:test_y})\n",
        "print('logit: ', _prob[:10], '\\nAccuracy: ', _accuracy)\n",
        "sess.close()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logit:  [[0.6077302 ]\n",
            " [0.46143475]\n",
            " [0.46142948]\n",
            " [0.20072547]\n",
            " [0.9743958 ]\n",
            " [0.6593118 ]\n",
            " [0.35063604]\n",
            " [0.46954417]\n",
            " [0.4363027 ]\n",
            " [0.99743146]] \n",
            "Accuracy:  0.6042524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aCjm2BNHc5c",
        "colab_type": "text"
      },
      "source": [
        "# Appendix: And OR Gate 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erU6IJ1OHp8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title 필요 Package import\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_random_seed(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Xwgju9zJw1",
        "colab_type": "text"
      },
      "source": [
        "# 정답\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N1f0-myzg0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title 데이터 준비\n",
        "\n",
        "num_epochs = 2000\n",
        "learning_rate = 1e-2\n",
        "\n",
        "x_data = [ [1., 0., 0.], # bias를위한 1 추가\n",
        "          [1., 0., 1.],\n",
        "          [1., 1., 0.],\n",
        "          [1., 1., 1.]]\n",
        "\n",
        "y_data_and = [[0], [0], [0], [1]]\n",
        "y_data_or = [[0], [1], [1], [1]]\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lxdPs6t0AC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title 모델 생성\n",
        "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "# model prediction \n",
        "logit = tf.matmul(X, W) + b\n",
        "prob = tf.sigmoid(logit)\n",
        "\n",
        "# Loss 및 학습법 정의\n",
        "epsilon = 1e-10\n",
        "loss = -tf.reduce_mean(Y * tf.log(prob + epsilon) + (1 - Y)*tf.log((1-prob) + epsilon))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate= learning_rate).minimize(loss)\n",
        "\n",
        "#평가 방법\n",
        "pred = tf.cast(prob > 0.5, dtype=tf.float32)\n",
        "correct = tf.cast(tf.equal(pred, Y), dtype=tf.float32)\n",
        "Accuracy = tf.reduce_mean(correct)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD4kdbd50Ang",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "0c360636-75d2-4102-ed91-5062d0bb446c"
      },
      "source": [
        "#@title 모델 학습\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(1, num_epochs+1):\n",
        "    loss_val, training = sess.run([loss, train], feed_dict={X: x_data, Y: y_data_and})\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print('step: ', step, 'loss: ', loss_val)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step:  100 loss:  0.75107586\n",
            "step:  200 loss:  0.7056655\n",
            "step:  300 loss:  0.66839194\n",
            "step:  400 loss:  0.63560486\n",
            "step:  500 loss:  0.60594845\n",
            "step:  600 loss:  0.57883227\n",
            "step:  700 loss:  0.5539292\n",
            "step:  800 loss:  0.5310072\n",
            "step:  900 loss:  0.5098734\n",
            "step:  1000 loss:  0.4903569\n",
            "step:  1100 loss:  0.4723027\n",
            "step:  1200 loss:  0.45557034\n",
            "step:  1300 loss:  0.44003272\n",
            "step:  1400 loss:  0.4255755\n",
            "step:  1500 loss:  0.41209602\n",
            "step:  1600 loss:  0.39950234\n",
            "step:  1700 loss:  0.38771248\n",
            "step:  1800 loss:  0.3766529\n",
            "step:  1900 loss:  0.36625838\n",
            "step:  2000 loss:  0.3564703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_cszQun2Hjx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "cf45f360-680c-4fcd-f946-adfdab4eaa11"
      },
      "source": [
        "#@title 모델 평가\n",
        "_prob, _pred, _accuracy = sess.run([prob, pred, Accuracy], feed_dict={X: x_data, Y:y_data_and})\n",
        "print('logit: ', _prob, '\\nAccuracy: ', _accuracy)\n",
        "sess.close()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logit:  [[0.11341632]\n",
            " [0.2917149 ]\n",
            " [0.23937859]\n",
            " [0.5032877 ]] \n",
            "Accuracy:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOteAd5B2sGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}