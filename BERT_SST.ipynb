{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Practice.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiya906/my-machine-learning/blob/master/BERT_SST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkH9llitpRMk",
        "colab_type": "text"
      },
      "source": [
        "# **사용법**\n",
        "\n",
        "1.   우측 상단 '로그인'\n",
        "2.   좌측 상단 '실습 모드에서 열기'\n",
        "\n",
        "\n",
        "※ 각각의 셀은 셀 좌측 상단 실행 버튼을 통해 실행할 수 있습니다.\n",
        "\n",
        "※ 실행 중 '경고: 이 노트는 Google에서 작성하지 않았습니다.'라는 창이 뜰 경우, '실행 전에 모든 런타임 재설정'란에 체크 후 '무시하고 계속하기'를 하시면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCe_BuvmQ4PM",
        "colab_type": "text"
      },
      "source": [
        "# 1. 사전 준비\n",
        "\n",
        "- Google drive 연동\n",
        "- 파일 복사\n",
        "- 필요한 Package 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vZpUXmHixgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_DIR = '/content/drive/My Drive/AI/SDS_BERT'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MylzFGXIVmw",
        "colab_type": "code",
        "outputId": "ac6bc99a-c57d-4c27-af33-4ca4dfde1dfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw0M0acjJrle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/drive/\"My Drive\"/AI/SDS_BERT/. ./"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeguRUJ2J5Hc",
        "colab_type": "code",
        "outputId": "680b7a45-d648-4470-c2db-3f074cd1bce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "!pip install funcsigs"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Installing collected packages: funcsigs\n",
            "Successfully installed funcsigs-1.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbAcovlwQ9ex",
        "colab_type": "text"
      },
      "source": [
        "# 2. 모델 및 데이터 다운로드 & 전처리\n",
        "\n",
        "- Pretrained BERT 다운로드\n",
        "- Dataset 다운로드\n",
        "- 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPXotKYzLzhn",
        "colab_type": "code",
        "outputId": "4a059a4f-7368-4bf6-cc41-d265f873d194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "!sh bert_pretrained_models/download_model.sh"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-23 07:38:03--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘bert_pretrained_models/uncased_L-12_H-768_A-12.zip.1’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   214MB/s    in 1.8s    \n",
            "\n",
            "2019-07-23 07:38:05 (214 MB/s) - ‘bert_pretrained_models/uncased_L-12_H-768_A-12.zip.1’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  bert_pretrained_models/uncased_L-12_H-768_A-12.zip\n",
            "replace bert_pretrained_models/uncased_L-12_H-768_A-12/bert_model.ckpt.meta? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: bert_pretrained_models/uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "replace bert_pretrained_models/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: bert_pretrained_models/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "replace bert_pretrained_models/uncased_L-12_H-768_A-12/vocab.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: bert_pretrained_models/uncased_L-12_H-768_A-12/vocab.txt  \n",
            "replace bert_pretrained_models/uncased_L-12_H-768_A-12/bert_model.ckpt.index? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: bert_pretrained_models/uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "replace bert_pretrained_models/uncased_L-12_H-768_A-12/bert_config.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: bert_pretrained_models/uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABb6VKxp388B",
        "colab_type": "code",
        "outputId": "4bdbf2bd-da55-4077-f077-084d7fb1216e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# 여기에서 선택 : COLA, SST, MRPC, MNLI, XNLI\n",
        "!python data/download_glue_data.py --tasks=SST"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading and extracting SST...\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O22yGdMdQtRP",
        "colab_type": "code",
        "outputId": "947125cb-8bbb-4290-d2a1-bad0fe158682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "# 여기에서 선택 : CoLA, SST, MRPC, MNLI, XNLI\n",
        "!python prepare_data.py --task=SST"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0723 07:38:46.132593 139924813449088 deprecation_wrapper.py:119] From /content/texar/core/layers.py:629: The name tf.layers.Layer is deprecated. Please use tf.compat.v1.layers.Layer instead.\n",
            "\n",
            "W0723 07:38:46.132965 139924813449088 deprecation_wrapper.py:119] From /content/texar/core/layers.py:682: The name tf.layers.MaxPooling1D is deprecated. Please use tf.compat.v1.layers.MaxPooling1D instead.\n",
            "\n",
            "W0723 07:38:46.133079 139924813449088 deprecation_wrapper.py:119] From /content/texar/core/layers.py:683: The name tf.layers.AveragePooling1D is deprecated. Please use tf.compat.v1.layers.AveragePooling1D instead.\n",
            "\n",
            "W0723 07:38:46.133286 139924813449088 deprecation_wrapper.py:119] From /content/texar/core/layers.py:1174: The name tf.layers.Conv1D is deprecated. Please use tf.compat.v1.layers.Conv1D instead.\n",
            "\n",
            "W0723 07:38:46.133417 139924813449088 deprecation_wrapper.py:119] From /content/texar/core/layers.py:1175: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n",
            "\n",
            "W0723 07:38:46.137324 139924813449088 deprecation_wrapper.py:119] From /content/texar/core/optimization.py:483: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0723 07:38:46.247575 139924813449088 deprecation_wrapper.py:119] From prepare_data.py:53: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0723 07:38:46.247744 139924813449088 deprecation_wrapper.py:119] From prepare_data.py:53: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0723 07:38:46.247856 139924813449088 deprecation_wrapper.py:119] From prepare_data.py:60: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0723 07:38:46.247919 139924813449088 prepare_data.py:60] Loading data\n",
            "W0723 07:38:46.248094 139924813449088 deprecation_wrapper.py:119] From /content/texar/utils/utils_io.py:226: The name tf.gfile.IsDirectory is deprecated. Please use tf.io.gfile.isdir instead.\n",
            "\n",
            "W0723 07:38:46.248314 139924813449088 deprecation_wrapper.py:119] From /content/utils/data_utils.py:80: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "I0723 07:38:47.499253 139924813449088 prepare_data.py:91] num_classes:2; num_train_data:67349\n",
            "W0723 07:38:47.499537 139924813449088 deprecation_wrapper.py:119] From /content/utils/tokenization.py:41: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0723 07:38:48.964903 139924813449088 deprecation_wrapper.py:119] From /content/utils/data_utils.py:499: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "I0723 07:39:24.597001 139924813449088 prepare_data.py:140] config_data.py has been updated\n",
            "I0723 07:39:24.597205 139924813449088 prepare_data.py:144] Data preparation finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYhSQjPPRYnT",
        "colab_type": "text"
      },
      "source": [
        "# 3. 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdMmR-E6RkLT",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. 학습 설정\n",
        "\n",
        "- 학습 설정 정의 및 수정\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aApjQa28pEnz",
        "colab_type": "code",
        "outputId": "dbae3e4e-8149-40e9-911a-08e3b854e57f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 여기에서 선택 : CoLA, SST-2, MRPC, MNLI, XNLI\n",
        "import config_func\n",
        "config_func.modify_task_dir('./data/SST-2')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0723 07:43:13.524230 140617375025024 config_func.py:24] config_data.py has been updated\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZimgpHikL_rM",
        "colab_type": "code",
        "outputId": "5afc05e9-4992-40aa-ebea-e2dbab1e7389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import importlib\n",
        "import tensorflow as tf\n",
        "import texar as tx\n",
        "\n",
        "from utils import model_utils\n",
        "\n",
        "# pylint: disable=invalid-name, too-many-locals, too-many-statements\n",
        "\n",
        "flags = tf.flags\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "FLAGS.remove_flag_values(FLAGS.flag_values_dict())\n",
        "\n",
        "flags.DEFINE_string('f', '', 'kernel')\n",
        "flags.DEFINE_bool(\"logtostderr\", False, \"Whether to run training.\")\n",
        "flags.DEFINE_string('stderrthreshold', 'fatal', 'log messages at this level, or more severe, to stderr in addition to the logfile.')\n",
        "flags.DEFINE_boolean('showprefixforinfo', True,\n",
        "                     'If False, do not prepend prefix to info messages '\n",
        "                     'when it\\'s logged to stderr, '\n",
        "                     '--verbosity is set to INFO level, '\n",
        "                     'and python logging is used.')\n",
        "flags.DEFINE_boolean('alsologtostderr',\n",
        "                     False,\n",
        "                     'also log to stderr?', allow_override_cpp=True)\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"config_bert_pretrain\", 'uncased_L-12_H-768_A-12',\n",
        "    \"The architecture of pre-trained BERT model to use.\")\n",
        "flags.DEFINE_string(\n",
        "    \"config_format_bert\", \"json\",\n",
        "    \"The configuration format. Set to 'json' if the BERT config file is in \"\n",
        "    \"the same format of the official BERT config file. Set to 'texar' if the \"\n",
        "    \"BERT config file is in Texar format.\")\n",
        "flags.DEFINE_string(\n",
        "    \"config_downstream\", \"config_classifier\",\n",
        "    \"Configuration of the downstream part of the model and optmization.\")\n",
        "flags.DEFINE_string(\n",
        "    \"config_data\", \"config_data\",\n",
        "    \"The dataset config.\")\n",
        "flags.DEFINE_string(\n",
        "    \"output_dir\", \"output/\",\n",
        "    \"The output directory where the model checkpoints will be written.\")\n",
        "flags.DEFINE_string(\n",
        "    \"checkpoint\", None,\n",
        "    \"Path to a model checkpoint (including bert modules) to restore from.\")\n",
        "flags.DEFINE_bool(\"do_train\", True, \"Whether to run training.\")\n",
        "flags.DEFINE_bool(\"do_eval\", False, \"Whether to run eval on the dev set.\")\n",
        "flags.DEFINE_bool(\"do_test\", False, \"Whether to run test on the test set.\")\n",
        "flags.DEFINE_bool(\"distributed\", False, \"Whether to run in distributed mode.\")\n",
        "\n",
        "config_data = importlib.import_module('config_data')\n",
        "config_downstream = importlib.import_module(\"config_classifier\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0723 07:44:19.604691 140056818276224 deprecation_wrapper.py:119] From /content/texar/core/layers.py:629: The name tf.layers.Layer is deprecated. Please use tf.compat.v1.layers.Layer instead.\n",
            "\n",
            "W0723 07:44:19.606336 140056818276224 deprecation_wrapper.py:119] From /content/texar/core/layers.py:682: The name tf.layers.MaxPooling1D is deprecated. Please use tf.compat.v1.layers.MaxPooling1D instead.\n",
            "\n",
            "W0723 07:44:19.607319 140056818276224 deprecation_wrapper.py:119] From /content/texar/core/layers.py:683: The name tf.layers.AveragePooling1D is deprecated. Please use tf.compat.v1.layers.AveragePooling1D instead.\n",
            "\n",
            "W0723 07:44:19.621073 140056818276224 deprecation_wrapper.py:119] From /content/texar/core/layers.py:1174: The name tf.layers.Conv1D is deprecated. Please use tf.compat.v1.layers.Conv1D instead.\n",
            "\n",
            "W0723 07:44:19.623011 140056818276224 deprecation_wrapper.py:119] From /content/texar/core/layers.py:1175: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n",
            "\n",
            "W0723 07:44:19.627622 140056818276224 deprecation_wrapper.py:119] From /content/texar/core/optimization.py:483: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lf1Jpa9RoAL",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. 모델 불러오기\n",
        "\n",
        "- BERT 설정 읽기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfIJseuyM0lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LOAD MODEL\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "bert_pretrain_dir = ('bert_pretrained_models''/%s') % FLAGS.config_bert_pretrain\n",
        "# Loads BERT model configuration\n",
        "if FLAGS.config_format_bert == \"json\":\n",
        "    bert_config = model_utils.transform_bert_to_texar_config(os.path.join(bert_pretrain_dir, 'bert_config.json'))\n",
        "elif FLAGS.config_format_bert == 'texar':\n",
        "    bert_config = importlib.import_module(('bert_config_lib.''config_model_%s') % FLAGS.config_bert_pretrain)\n",
        "else:\n",
        "    raise ValueError('Unknown config_format_bert.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFs2ML8zRqA-",
        "colab_type": "text"
      },
      "source": [
        "## 3.3. 데이터 불러오기\n",
        "\n",
        "- 데이터 읽고 준비하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C60iNnS7PZEC",
        "colab_type": "code",
        "outputId": "b6f2dd50-c5e0-467d-b29b-56b766a05e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "# Loads data\n",
        "num_classes = config_data.num_classes\n",
        "num_train_data = config_data.num_train_data\n",
        "\n",
        "train_dataset = tx.data.TFRecordData(hparams=config_data.train_hparam)\n",
        "eval_dataset = tx.data.TFRecordData(hparams=config_data.eval_hparam)\n",
        "test_dataset = tx.data.TFRecordData(hparams=config_data.test_hparam)\n",
        "\n",
        "iterator = tx.data.FeedableDataIterator({\n",
        "    'train': train_dataset, 'eval': eval_dataset, 'test': test_dataset})\n",
        "batch = iterator.get_next()\n",
        "input_ids = batch[\"input_ids\"]\n",
        "segment_ids = batch[\"segment_ids\"]\n",
        "batch_size = tf.shape(input_ids)[0]\n",
        "input_length = tf.reduce_sum(1 - tf.to_int32(tf.equal(input_ids, 0)), axis=1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0723 07:44:24.011317 140056818276224 deprecation_wrapper.py:119] From /content/texar/data/data_decoders.py:603: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0723 07:44:24.012494 140056818276224 deprecation_wrapper.py:119] From /content/texar/data/data_decoders.py:610: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "W0723 07:44:24.035303 140056818276224 deprecation.py:323] From /content/texar/data/data/data_base.py:161: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n",
            "W0723 07:44:24.036144 140056818276224 deprecation.py:323] From /content/texar/data/data/data_base.py:162: padded_batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.padded_batch(..., drop_remainder=True)`.\n",
            "W0723 07:44:24.107301 140056818276224 deprecation_wrapper.py:119] From /content/texar/utils/variables.py:50: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0723 07:44:24.108512 140056818276224 deprecation_wrapper.py:119] From /content/texar/data/data/data_iterators.py:326: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0723 07:44:24.116523 140056818276224 deprecation_wrapper.py:119] From /content/texar/data/data/data_iterators.py:328: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
            "\n",
            "W0723 07:44:24.117447 140056818276224 deprecation.py:323] From /content/texar/data/data/data_iterators.py:329: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_types(dataset)`.\n",
            "W0723 07:44:24.123500 140056818276224 deprecation.py:323] From /content/texar/data/data/data_iterators.py:334: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "W0723 07:44:24.147618 140056818276224 deprecation.py:323] From <ipython-input-3-2eb7497705ee>:14: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_6AHhZIRsWc",
        "colab_type": "text"
      },
      "source": [
        "## 3.4. BERT 완성하기\n",
        "\n",
        "- BERT 코드 TEXAR 이용하여 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pYXsG_CP-At",
        "colab_type": "code",
        "outputId": "7dc0e23a-39e2-4b71-aeff-78f77254ba3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "# Builds BERT\n",
        "with tf.variable_scope('bert'):\n",
        "    # Word embedding\n",
        "    embedder = tx.modules.WordEmbedder(\n",
        "        vocab_size=30522, hparams=bert_config.embed)\n",
        "    word_embeds = embedder(input_ids)\n",
        "\n",
        "    # Segment embedding for each type of tokens\n",
        "    segment_embedder = tx.modules.WordEmbedder(\n",
        "        vocab_size=bert_config.type_vocab_size,\n",
        "        hparams=bert_config.segment_embed)\n",
        "    segment_embeds = segment_embedder(segment_ids)\n",
        "\n",
        "    # Position embedding\n",
        "    position_embedder = tx.modules.PositionEmbedder(\n",
        "        position_size=bert_config.position_size,\n",
        "        hparams=bert_config.position_embed)\n",
        "    seq_length = tf.ones([batch_size], tf.int32) * tf.shape(input_ids)[1]\n",
        "    pos_embeds = position_embedder(sequence_length=seq_length)\n",
        "    \n",
        "    # ============= Aggregates embeddings =============\n",
        "    \n",
        "    input_embeds = word_embeds + segment_embeds + pos_embeds\n",
        "    \n",
        "    # =================================================\n",
        "\n",
        "    # The BERT model (a TransformerEncoder)\n",
        "    # input_embeds: (batch, seq_len, emb_dim)\n",
        "    # output      : (batch, seq_len, emb_dim)\n",
        "    encoder = tx.modules.TransformerEncoder(hparams=bert_config.encoder)\n",
        "    output = encoder(input_embeds, input_length)\n",
        "\n",
        "    # Builds layers for downstream classification, which is also\n",
        "    # initialized with BERT pre-trained checkpoint.\n",
        "    with tf.variable_scope(\"pooler\"):\n",
        "        # Uses the projection of the 1st-step hidden vector of BERT output\n",
        "        # as the representation of the sentence\n",
        "\n",
        "        # ============= Get sentence embedding (CLS) =============\n",
        "        # CLS_hidden : First vector from contextual embedding\n",
        "        CLS_hidden = tf.squeeze(output[:, 0:1, :], axis=1)\n",
        "    \n",
        "        # ========================================================\n",
        "        \n",
        "        # Output\n",
        "        bert_sent_output = tf.layers.dense(CLS_hidden, config_downstream.hidden_dim, activation=tf.tanh)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0723 07:44:26.081236 140056818276224 deprecation_wrapper.py:119] From /content/texar/module_base.py:72: The name tf.make_template is deprecated. Please use tf.compat.v1.make_template instead.\n",
            "\n",
            "W0723 07:44:26.092065 140056818276224 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0723 07:44:26.279551 140056818276224 deprecation_wrapper.py:119] From /usr/lib/python3.6/pydoc.py:1595: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
            "\n",
            "W0723 07:44:26.288738 140056818276224 deprecation_wrapper.py:119] From /content/texar/core/layers.py:598: The name tf.layers.Layer is deprecated. Please use tf.compat.v1.layers.Layer instead.\n",
            "\n",
            "W0723 07:44:26.423269 140056818276224 deprecation.py:323] From /content/texar/modules/encoders/transformer_encoders.py:313: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "W0723 07:44:27.003502 140056818276224 deprecation_wrapper.py:119] From /content/texar/modules/networks/network_base.py:122: The name tf.layers.Dropout is deprecated. Please use tf.compat.v1.layers.Dropout instead.\n",
            "\n",
            "W0723 07:44:27.004839 140056818276224 deprecation_wrapper.py:119] From /content/texar/modules/networks/network_base.py:123: The name tf.layers.BatchNormalization is deprecated. Please use tf.compat.v1.layers.BatchNormalization instead.\n",
            "\n",
            "W0723 07:44:30.993251 140056818276224 deprecation.py:323] From <ipython-input-4-24fd42f1c205>:45: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxA23dyLRzhG",
        "colab_type": "text"
      },
      "source": [
        "## 3.5. Classification Layer 구현하기\n",
        "\n",
        "- Contextural Embedding ('bert_sent_output') 을 활용하여 Classification을 위한 Layer를 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P4-lsEoBpDM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "de36fe4e-28ab-4f8c-e4cc-5dab68dcb338"
      },
      "source": [
        "# Adds the final classification layer\n",
        "\n",
        "# Hint: USE \"tf.layers.dense(dense_input, num_classes)\"\n",
        "# output X W --> logits\n",
        "logits = tf.layers.dense(bert_sent_output, num_classes)\n",
        "\n",
        "# Argmax over last dimension\n",
        "preds = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "\n",
        "# Accuracy\n",
        "accu = tx.evals.accuracy(batch['label_ids'], preds)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0723 07:44:33.298686 140056818276224 deprecation.py:323] From /content/texar/evals/metrics.py:29: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQPXotIhfy2g",
        "colab_type": "text"
      },
      "source": [
        "## 3.4, 3.5 정답"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vYHxya8f52Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "5d268840-246b-46c8-d0e3-38c98a1b2363"
      },
      "source": [
        "# Builds BERT\n",
        "with tf.variable_scope('bert'):\n",
        "    # Word embedding\n",
        "    embedder = tx.modules.WordEmbedder(\n",
        "        vocab_size=30522, hparams=bert_config.embed)\n",
        "    word_embeds = embedder(input_ids)\n",
        "\n",
        "    # Segment embedding for each type of tokens\n",
        "    segment_embedder = tx.modules.WordEmbedder(\n",
        "        vocab_size=bert_config.type_vocab_size,\n",
        "        hparams=bert_config.segment_embed)\n",
        "    segment_embeds = segment_embedder(segment_ids)\n",
        "\n",
        "    # Position embedding\n",
        "    position_embedder = tx.modules.PositionEmbedder(\n",
        "        position_size=bert_config.position_size,\n",
        "        hparams=bert_config.position_embed)\n",
        "    seq_length = tf.ones([batch_size], tf.int32) * tf.shape(input_ids)[1]\n",
        "    pos_embeds = position_embedder(sequence_length=seq_length)\n",
        "\n",
        "    # Aggregates embeddings\n",
        "    input_embeds = word_embeds + segment_embeds + pos_embeds\n",
        "\n",
        "    # The BERT model (a TransformerEncoder)\n",
        "    # input_embeds: (batch, seq_len, emb_dim)\n",
        "    # output      : (batch, seq_len, emb_dim)\n",
        "    encoder = tx.modules.TransformerEncoder(hparams=bert_config.encoder)\n",
        "    output = encoder(input_embeds, input_length)\n",
        "\n",
        "    # Builds layers for downstream classification, which is also\n",
        "    # initialized with BERT pre-trained checkpoint.\n",
        "    with tf.variable_scope(\"pooler\"):\n",
        "        # Uses the projection of the 1st-step hidden vector of BERT output\n",
        "        # as the representation of the sentence\n",
        "        \n",
        "        # CLS_hidden : First vector from contextual embedding\n",
        "        CLS_hidden = tf.squeeze(output[:, 0:1, :], axis=1)\n",
        "        \n",
        "        # Output\n",
        "        bert_sent_output = tf.layers.dense(CLS_hidden, config_downstream.hidden_dim, activation=tf.tanh)\n",
        "        \n",
        "# Adds the final classification layer\n",
        "\n",
        "# output X W --> logits\n",
        "# num_classes = 정답 클래스 개수\n",
        "logits = tf.layers.dense(bert_sent_output, num_classes)\n",
        "\n",
        "# Argmax over last dimension\n",
        "preds = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "\n",
        "accu = tx.evals.accuracy(batch['label_ids'], preds)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a7ebf72009b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Word embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     embedder = tx.modules.WordEmbedder(\n\u001b[0;32m----> 4\u001b[0;31m         vocab_size=30522, hparams=bert_config.embed)\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mword_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/texar/modules/embedders/embedders.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, init_value, vocab_size, hparams)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         self._init_parameterized_embedding(init_value, vocab_size,\n\u001b[0;32m--> 104\u001b[0;31m                                            self._hparams)\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/texar/modules/embedders/embedder_base.py\u001b[0m in \u001b[0;36m_init_parameterized_embedding\u001b[0;34m(self, init_value, num_embeds, hparams)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_parameterized_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         self._embedding = embedder_utils.get_embedding(\n\u001b[0;32m---> 53\u001b[0;31m             hparams, init_value, num_embeds, self.variable_scope)\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_trainable_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/texar/modules/embedders/embedder_utils.py\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(hparams, init_value, num_embeds, variable_scope)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                                         \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                         trainable=hparams[\"trainable\"])\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             embedding = tf.get_variable(name='w',\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1494\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1237\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    560\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    512\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     synchronization, aggregation, trainable = (\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 864\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    865\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable bert/word_embeddings/w already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/content/texar/modules/embedders/embedder_utils.py\", line 209, in get_embedding\n    trainable=hparams[\"trainable\"])\n  File \"/content/texar/modules/embedders/embedder_base.py\", line 53, in _init_parameterized_embedding\n    hparams, init_value, num_embeds, self.variable_scope)\n  File \"/content/texar/modules/embedders/embedders.py\", line 104, in __init__\n    self._hparams)\n  File \"<ipython-input-11-24fd42f1c205>\", line 4, in <module>\n    vocab_size=30522, hparams=bert_config.embed)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKdPfegtR4Tc",
        "colab_type": "text"
      },
      "source": [
        "## 3.6. Loss 및 Optimizer 설정\n",
        "\n",
        "- Loss, Optimizer 정의\n",
        "- Pretrained BERT 값을 불러와서 입히기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdlEt3ygW9SK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "outputId": "5487ccb9-9266-4619-81fe-1c6da8673a15"
      },
      "source": [
        "# Optimization\n",
        "loss = tf.losses.sparse_softmax_cross_entropy(labels=batch[\"label_ids\"], logits=logits)\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "# Learning rate\n",
        "static_lr = config_downstream.lr['static_lr']\n",
        "num_train_steps = int(num_train_data / config_data.train_batch_size * config_data.max_train_epoch)\n",
        "num_warmup_steps = int(num_train_steps * config_data.warmup_proportion)\n",
        "# lr is a Tensor\n",
        "lr = model_utils.get_lr(global_step, num_train_steps, num_warmup_steps, static_lr)\n",
        "\n",
        "# Optimize\n",
        "opt = tx.core.get_optimizer(\n",
        "    global_step=global_step,\n",
        "    learning_rate=lr,\n",
        "    hparams=config_downstream.opt\n",
        ")\n",
        "\n",
        "train_op = tf.contrib.layers.optimize_loss(\n",
        "        loss=loss,\n",
        "        global_step=global_step,\n",
        "        learning_rate=None,\n",
        "        optimizer=opt)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0723 07:44:39.067096 140056818276224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0723 07:44:39.085576 140056818276224 deprecation_wrapper.py:119] From /content/utils/model_utils.py:86: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0723 07:44:39.093857 140056818276224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0723 07:44:39.111694 140056818276224 deprecation_wrapper.py:119] From /content/texar/core/optimization.py:405: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0723 07:44:49.451478 140056818276224 deprecation.py:323] From /content/texar/core/optimization.py:570: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIPJGW8DXK5o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "fb304ab6-5399-49e9-94ce-039ebb75e2c3"
      },
      "source": [
        "# Loads pretrained BERT model parameters\n",
        "init_checkpoint = os.path.join(bert_pretrain_dir, 'bert_model.ckpt')\n",
        "model_utils.init_bert_checkpoint(init_checkpoint)\n",
        "\n",
        "session_config = tf.ConfigProto()\n",
        "\n",
        "sess = tf.Session(config=session_config)\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "sess.run(tf.local_variables_initializer())\n",
        "sess.run(tf.tables_initializer())\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "if FLAGS.checkpoint:\n",
        "    saver.restore(sess, FLAGS.checkpoint)\n",
        "    \n",
        "iterator.initialize_dataset(sess)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0723 07:44:53.948428 140056818276224 deprecation_wrapper.py:119] From /content/utils/model_utils.py:187: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omA-ScoySGpm",
        "colab_type": "text"
      },
      "source": [
        "## 3.7. 평가 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYnOxZZ4fA3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _eval_epoch(sess):\n",
        "    \"\"\"Evaluates on the dev set.\n",
        "    \"\"\"\n",
        "    iterator.restart_dataset(sess, 'eval')\n",
        "\n",
        "    cum_acc = 0.0\n",
        "    cum_loss = 0.0\n",
        "    nsamples = 0\n",
        "    fetches = {\n",
        "        'accu': accu,\n",
        "        'loss': loss,\n",
        "        'batch_size': batch_size,\n",
        "    }\n",
        "    while True:\n",
        "        try:\n",
        "            feed_dict = {\n",
        "                iterator.handle: iterator.get_handle(sess, 'eval'),\n",
        "                tx.context.global_mode(): tf.estimator.ModeKeys.EVAL,\n",
        "            }\n",
        "            rets = sess.run(fetches, feed_dict)\n",
        "\n",
        "            cum_acc += rets['accu'] * rets['batch_size']\n",
        "            cum_loss += rets['loss'] * rets['batch_size']\n",
        "            nsamples += rets['batch_size']\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            break\n",
        "    tf.logging.info('eval accu: {}; loss: {}; nsamples: {}'.format(\n",
        "        cum_acc / nsamples, cum_loss / nsamples, nsamples))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0PSNezaSK-4",
        "colab_type": "text"
      },
      "source": [
        "## 3.8. 학습 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6EK68u1Xv3f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "979d6f00-bb8b-4800-a727-daf909e00727"
      },
      "source": [
        "for i in range(config_data.max_train_epoch):\n",
        "    # Dataset 초기화\n",
        "    iterator.restart_dataset(sess, 'train')\n",
        "    fetches = {\n",
        "            'train_op': train_op,\n",
        "            'loss': loss,\n",
        "            'batch_size': batch_size,\n",
        "            'step': global_step\n",
        "        }\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            feed_dict = {\n",
        "                iterator.handle: iterator.get_handle(sess, 'train'),\n",
        "                tx.global_mode(): tf.estimator.ModeKeys.TRAIN,\n",
        "            }\n",
        "            rets = sess.run(fetches, feed_dict)\n",
        "            step = rets['step']\n",
        "\n",
        "            dis_steps = config_data.display_steps\n",
        "            if dis_steps > 0 and step % dis_steps == 0:\n",
        "                tf.logging.info('step:%d; loss:%f' % (step, rets['loss']))\n",
        "\n",
        "            eval_steps = config_data.eval_steps\n",
        "            if eval_steps > 0 and step % eval_steps == 0:\n",
        "                _eval_epoch(sess)\n",
        "\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            break\n",
        "saver.save(sess, FLAGS.output_dir + '/model.ckpt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0723 07:45:57.263368 140056818276224 <ipython-input-9-b2ea9810eb12>:22] step:50; loss:0.612986\n",
            "I0723 07:46:39.917129 140056818276224 <ipython-input-9-b2ea9810eb12>:22] step:100; loss:0.328315\n",
            "I0723 07:46:49.315532 140056818276224 <ipython-input-8-7129b0313312>:28] eval accu: 0.8658256880733946; loss: 0.32551266106033544; nsamples: 872\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b2ea9810eb12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             }\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mrets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsN2l2t7DjYY",
        "colab_type": "text"
      },
      "source": [
        "## 3.9. Test Data 예측 결과 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy_3RK_FYEm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test Data 읽고 예측\n",
        "\n",
        "import csv\n",
        "test_file = os.path.join(config_data.tfrecord_data_dir, 'test.tsv')\n",
        "lines = []\n",
        "with tf.gfile.Open(test_file, \"r\") as f:\n",
        "    reader = csv.reader(f, delimiter=\"\\t\")\n",
        "    for i, line in enumerate(reader):\n",
        "        if i == 0:\n",
        "            continue\n",
        "        if len(line) > 3:\n",
        "            lines.append(line[-2:])\n",
        "        else:\n",
        "            lines.append(line[-1])\n",
        "\n",
        "iterator.restart_dataset(sess, 'test')\n",
        "\n",
        "_all_preds = []\n",
        "while True:\n",
        "    try:\n",
        "        feed_dict = {\n",
        "            iterator.handle: iterator.get_handle(sess, 'test'),\n",
        "            tx.context.global_mode(): tf.estimator.ModeKeys.PREDICT,\n",
        "        }\n",
        "        _preds = sess.run(preds, feed_dict=feed_dict)\n",
        "        _all_preds.extend(_preds.tolist())\n",
        "    except tf.errors.OutOfRangeError:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vawccb64cIke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "90ec36d8-6727-4d58-af65-1138306d5611"
      },
      "source": [
        "# 몇 개의 Test 샘플을 출력할 것인가\n",
        "NUM_PRINT_TEST = 40\n",
        "\n",
        "for i in range(NUM_PRINT_TEST):\n",
        "  if isinstance(lines[i], list):\n",
        "      print('Sentence 1: ', lines[i][0])\n",
        "      print('Sentence 2: ', lines[i][1])\n",
        "      print('Prediction: ', _all_preds[i])\n",
        "  else:\n",
        "      print('Sentence  : ', lines[i])\n",
        "      print('Prediction: ', _all_preds[i])\n",
        "      \n",
        "  print()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence  :  uneasy mishmash of styles and genres .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  this film 's relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  director rob marshall went out gunning to make a great one .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  lathan and diggs have considerable personal charm , and their screen rapport makes the old story seem new .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  a well-made and often lovely depiction of the mysteries of friendship .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  none of this violates the letter of behan 's book , but missing is its spirit , its ribald , full-throated humor .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  although it bangs a very cliched drum at times , this crowd-pleaser 's fresh dialogue , energetic music , and good-natured spunk are often infectious .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  this is junk food cinema at its greasiest .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  it 's also heavy-handed and devotes too much time to bigoted views .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  it helps that lil bow wow ... tones down his pint-sized gangsta act to play someone who resembles a real kid .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  watching the film is like reading a times portrait of grief that keeps shifting focus to the journalist who wrote it .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  moore 's performance impresses almost as much as her work with haynes in 1995 's safe .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  reinforces the talents of screenwriter charlie kaufman , creator of adaptation and being john malkovich .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  now trimmed by about 20 minutes , this lavish three-year-old production has enough grandeur and scale to satisfy as grown-up escapism .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  a journey through memory , a celebration of living , and a sobering rumination on fatality , classism , and ignorance .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  a remarkable 179-minute meditation on the nature of revolution .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  waydowntown is by no means a perfect film , but its boasts a huge charm factor and smacks of originality .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  it 's just incredibly dull .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  some , like ballistic , arrive stillborn ... looking like the beaten , well-worn video box cover of seven years into the future .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  a feel-good picture in the best sense of the term .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  mark me down as a non-believer in werewolf films that are not serious and rely on stupidity as a substitute for humor .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  good movie .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  with spy kids 2 : the island of lost dreams writer/director/producer robert rodriguez has cobbled together a film that feels like a sugar high gone awry .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  starts off with a bang , but then fizzles like a wet stick of dynamite at the very end .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  the best way to hope for any chance of enjoying this film is by lowering your expectations .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  while the mystery surrounding the nature of the boat 's malediction remains intriguing enough to sustain mild interest , the picture refuses to offer much accompanying sustenance in the way of characterization , humor or plain old popcorn fun .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  this is a startling film that gives you a fascinating , albeit depressing view of iranian rural life close to the iraqi border .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  here 's a british flick gleefully unconcerned with plausibility , yet just as determined to entertain you .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  most new movies have a bright sheen .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  so few movies explore religion that it 's disappointing to see one reduce it to an idea that fits in a sampler .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  nothing more than a mediocre trifle .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  a model of what films like this should be like .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  gangs of new york is an unapologetic mess , whose only saving grace is that it ends by blowing just about everything up .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  doug liman , the director of bourne , directs the traffic well , gets a nice wintry look from his locations , absorbs us with the movie 's spycraft and uses damon 's ability to be focused and sincere .\n",
            "Prediction:  1\n",
            "\n",
            "Sentence  :  another big , dumb action movie in the vein of xxx , the transporter is riddled with plot holes big enough for its titular hero to drive his sleek black bmw through .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  it becomes gimmicky instead of compelling .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  falters when it takes itself too seriously and when it depends too heavily on its otherwise talented cast to clown in situations that are n't funny .\n",
            "Prediction:  0\n",
            "\n",
            "Sentence  :  verbinski substitutes atmosphere for action , tedium for thrills .\n",
            "Prediction:  1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}