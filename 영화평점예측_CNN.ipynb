{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "main_colab_CNN_실습중.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiya906/my-machine-learning/blob/master/%EC%98%81%ED%99%94%ED%8F%89%EC%A0%90%EC%98%88%EC%B8%A1_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGL8ad1gWyoi",
        "colab_type": "text"
      },
      "source": [
        "# SDS NLP Challenge\n",
        "\n",
        "- 이 Challenge는 자연어 처리를 통해 영화 감상평을 보고 영화 평점을 예측하는 것을 목적으로 합니다.\n",
        "- 데이터는 2 종류가 있습니다: IMDB (영어), NSMC (한국어)\n",
        "- 전처리, 학습 및 평가 템플릿은 아래와 같이 제공됩니다. 원하시는 대로 Parameter를 조절하고 'MyModel'에 해당하는 본인의 모델을 구현하여 실험하시면 됩니다.\n",
        "- Baseline 성능은 XXX와 같습니다.\n",
        "- Challenge 설명 PPT에 더 자세한 설명이 들어있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VyuvW4lWyoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.mkdir('data')\n",
        "os.mkdir('models')\n",
        "os.mkdir('util')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voXFCp0GovHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "print(dir(tf.feature_column))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2yGa2OVWyom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORT PACKAGE\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from util.Data_loader import load_data\n",
        "from util.Dataset import Dataset\n",
        "from util.Parser import vocab_dictionary, parse_data\n",
        "\n",
        "from models.Baseline_LSTM import LSTM\n",
        "from models.Baseline_Char_CNN import Char_CNN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqmHdk-WWyop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imdb, nsmc\n",
        "DATA_NAME = 'imdb'\n",
        "\n",
        "# 전처리 방법\n",
        "# imdb (영어) = word \n",
        "# nsmc (한글) = eumjeol or char (음절 or 음소)\n",
        "PARSER = 'word'\n",
        "\n",
        "MAXLEN = 300\n",
        "# BATCH_SIZE = 256\n",
        "# LEARNING_RATE = 0.01\n",
        "BATCH_SIZE = 256\n",
        "LEARNING_RATE = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lwXXrmpWyoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file = DATA_NAME + '_train.txt'\n",
        "test_file = DATA_NAME + '_test.txt'\n",
        "train_comments, train_ratings, test_comments, test_ratings = load_data('data', train_file, test_file)\n",
        "\n",
        "str2idx = vocab_dictionary(train_comments, PARSER)\n",
        "idx2str = {i: s for s, i in str2idx.items()}\n",
        "\n",
        "CHARSIZE = len(str2idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88vFaON3Wyos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 전처리\n",
        "train_x = parse_data(train_comments, str2idx, PARSER)\n",
        "train_y = train_ratings\n",
        "\n",
        "test_x = parse_data(test_comments, str2idx, PARSER)\n",
        "test_y = test_ratings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz_rwAMCWyot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터셋\n",
        "train_dataset = Dataset(train_x, train_y, MAXLEN, BATCH_SIZE, shuffle=True)\n",
        "test_dataset = Dataset(test_x, test_y, MAXLEN, BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy0f9wLRiunG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class MyModel():\n",
        "    def __init__(self, embedding, conv_layers, fc_layers, maxlen, char_size, lr):\n",
        "        self.embedding = embedding\n",
        "        self.input_len = maxlen\n",
        "        self.char_size = char_size\n",
        "        self.conv_layers = conv_layers\n",
        "        self.fc_layers = fc_layers\n",
        "        self.lr = lr\n",
        "        \n",
        "        self.output_keep_prob = tf.placeholder(tf.float32, name='output_keep_prob')\n",
        "\n",
        "        with tf.name_scope(\"Input-Layer\"):\n",
        "            # Input\n",
        "            self.x = tf.placeholder(tf.int64, shape=[None, self.input_len], name=\"input_x\")\n",
        "            self.y = tf.placeholder(tf.float32, shape=[None], name=\"output_x\")\n",
        "            embedding_matrix = tf.Variable(tf.random_normal([self.char_size, self.embedding], stddev=0.01), name='Embedding_matrix')\n",
        "            print(\"Embedding Matrix: \", embedding_matrix.shape)\n",
        "\n",
        "        # EMBEDDING LAYERS\n",
        "        with tf.name_scope(\"Embedding-Layer\"):\n",
        "            cnn_x = tf.nn.embedding_lookup(embedding_matrix, self.x)\n",
        "            cnn_x = tf.expand_dims(cnn_x, -1)\n",
        "        \n",
        "        \n",
        "        # ================================ Version 2 =================================\n",
        "        # CONVOLUTION LAYERS\n",
        "        for i, conv_info in enumerate(self.conv_layers):\n",
        "            print(\"CNN Input: \", cnn_x.shape)\n",
        "            with tf.name_scope(\"Conv-Layer\" + str(i)):\n",
        "                filter_width = cnn_x.get_shape()[2].value\n",
        "                filter_shape = [conv_info[1], filter_width, 1, conv_info[0]]\n",
        "\n",
        "                W = tf.Variable(tf.random_normal(filter_shape,  mean=0.0, stddev=0.01), dtype=tf.float32,\n",
        "                                name='Conv_W')  # large = 0.02 , small = 0.05\n",
        "                b = tf.Variable(tf.random_normal(shape=[conv_info[0]],  mean=0.0, stddev=0.01), dtype=tf.float32,\n",
        "                                name='Conv_b')\n",
        "\n",
        "                conv = tf.nn.conv2d(cnn_x, W, [1, 1, 1, 1], \"VALID\", name=\"conv\")\n",
        "                cnn_x = tf.nn.bias_add(conv, b)\n",
        "\n",
        "            with tf.name_scope(\"Non-Linear\"):\n",
        "                cnn_x = tf.nn.relu(cnn_x)\n",
        "#                 cnn_x = tf.nn.tanh(cnn_x)\n",
        "            print(\"CNN Output: \", cnn_x.shape)\n",
        "            if conv_info[-1] != -1:\n",
        "                print(\"Pooling Input: \", cnn_x.shape)\n",
        "                with tf.name_scope(\"Max-Polling\"):\n",
        "                    pool_shape = [1, conv_info[-1], 1, 1]\n",
        "                    pool = tf.nn.max_pool(cnn_x, ksize=pool_shape, strides=pool_shape, padding=\"VALID\")\n",
        "                    cnn_x = tf.transpose(pool, [0, 1, 3, 2])\n",
        "                print(\"Pooling Output: \", cnn_x.shape)\n",
        "            else:\n",
        "                cnn_x = tf.transpose(cnn_x, [0, 1, 3, 2])\n",
        "        cnn_output = tf.squeeze(cnn_x, axis=3)\n",
        "\n",
        "        # Flatten cnn_output: (batch, height, width)  --> (batch, height * width)\n",
        "        out_shape = cnn_output.get_shape()\n",
        "        d = out_shape[1].value * out_shape[2].value\n",
        "        print('out_shape[1].value: ', out_shape[1].value)\n",
        "        print('out_shape[2].value: ', out_shape[2].value)\n",
        "        fc_input = tf.reshape(cnn_output, [-1, d])\n",
        "        print('fc_input: ', fc_input)\n",
        "\n",
        "        # Add dropout\n",
        "#         keep_prob = tf.placeholder(tf.float32)\n",
        "        h_drop = tf.nn.dropout(fc_input, self.output_keep_prob)\n",
        "        \n",
        "        # Regression layer\n",
        "        print(\"Output Layer input: \", h_drop.shape)\n",
        "        with tf.name_scope(\"Output-Layer\"):\n",
        "            W = tf.Variable(tf.random_normal([d, 1], stddev=0.01), name=\"Output_W\")\n",
        "            b = tf.Variable(tf.random_normal([1], stddev=0.01), name=\"Output_b\")\n",
        "\n",
        "            output = tf.squeeze(tf.matmul(h_drop, W) + b, 1)\n",
        "            self.pred = output\n",
        "        print(\"Output Layer output: \", output.shape)\n",
        "\n",
        "        with tf.name_scope(\"Loss\"):\n",
        "            squared_error = tf.square(self.y - output)\n",
        "            self.loss = tf.reduce_mean(squared_error)\n",
        "\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
        "        self.train_step = optimizer.minimize(self.loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVvmB8W41elf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 병렬 ##\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class MyModel2():\n",
        "    def __init__(self, embedding, conv_layers, fc_layers, maxlen, char_size, lr):\n",
        "        self.embedding = embedding\n",
        "        self.input_len = maxlen\n",
        "        self.char_size = char_size\n",
        "        self.conv_layers = conv_layers\n",
        "        self.fc_layers = fc_layers\n",
        "        self.lr = lr\n",
        "        \n",
        "        self.output_keep_prob = tf.placeholder(tf.float32, name='output_keep_prob')\n",
        "        \n",
        "        filter_sizes = [3]\n",
        "        sec_filter_sizes = [1,2,3]\n",
        "        num_filters = 32\n",
        "        hidden_layer_size = 1280\n",
        "\n",
        "        with tf.name_scope(\"Input-Layer\"):\n",
        "            # Input\n",
        "            self.x = tf.placeholder(tf.int64, shape=[None, self.input_len], name=\"input_x\")\n",
        "            self.y = tf.placeholder(tf.float32, shape=[None], name=\"output_x\")\n",
        "            embedding_matrix = tf.Variable(tf.random_normal([self.char_size, self.embedding], stddev=0.01), name='Embedding_matrix')\n",
        "            print(\"Embedding Matrix: \", embedding_matrix.shape)\n",
        "\n",
        "        # EMBEDDING LAYERS\n",
        "        with tf.name_scope(\"Embedding-Layer\"):\n",
        "            cnn_x = tf.nn.embedding_lookup(embedding_matrix, self.x)\n",
        "            cnn_x = tf.expand_dims(cnn_x, -1)\n",
        "        print('embedeed_cnn_x: ', cnn_x)\n",
        "        \n",
        "        pooled_outputs = []\n",
        "        \n",
        "        for j, sec_filter_size in enumerate(sec_filter_sizes):\n",
        "            with tf.name_scope(\"conv2-maxpool-%s\" % sec_filter_size):\n",
        "                # Convolution Layer\n",
        "                conv2 = tf.layers.conv2d(cnn_x,\n",
        "                                        filters=num_filters,\n",
        "                                        kernel_size=[sec_filter_size, 32],\n",
        "                                        strides=[1, 1],\n",
        "                                        activation=tf.nn.relu, )\n",
        "                # Maxpooling Layer\n",
        "                pooled2 = tf.layers.max_pooling2d(conv2,\n",
        "                                                 [conv2.shape[1],1],\n",
        "                                                 strides=[1, 1], )\n",
        "                pooled_outputs.append(pooled2)\n",
        "                print('pooled2 shape: ', pooled2.shape)\n",
        "                print('pooled_outputs shape: ', pooled_outputs)\n",
        "                print('conv2.shape[1]: ', conv2.shape[1])\n",
        "#=================================================================================#\n",
        "#         for i, filter_size in enumerate(filter_sizes):\n",
        "#           with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "            \n",
        "#             # Convolution Layer\n",
        "#             conv = tf.layers.conv2d(cnn_x,\n",
        "#                                     filters=num_filters,\n",
        "#                                     kernel_size=[filter_size, filter_size],\n",
        "#                                     strides=[1, 1],\n",
        "#                                     activation=tf.nn.relu,)\n",
        "\n",
        "#             # Maxpooling Layer\n",
        "#             pooled = tf.layers.max_pooling2d(conv,\n",
        "#                                     [3, 1],\n",
        "#                                     strides=[2, 1],)\n",
        "#             pooled = tf.transpose(pooled, [0, 1, 3, 2])\n",
        "\n",
        "#             for j, sec_filter_size in enumerate(sec_filter_sizes):\n",
        "#                 with tf.name_scope(\"conv2-maxpool-%s\" % sec_filter_size):\n",
        "#                     # Convolution Layer\n",
        "#                     conv2 = tf.layers.conv2d(pooled,\n",
        "#                                             filters=num_filters,\n",
        "#                                             kernel_size=[sec_filter_size, pooled.shape[2]],\n",
        "#                                             strides=[1, 1],\n",
        "#                                             activation=tf.nn.relu, )\n",
        "#                     print('sec_filter_size: ', sec_filter_size)\n",
        "#                     print('pooled_shape: ', pooled.shape[2])\n",
        "#                     # Maxpooling Layer\n",
        "#                     pooled2 = tf.layers.max_pooling2d(conv2,\n",
        "#                                                      [conv2.shape[1], 1],\n",
        "#                                                      strides=[1, 1], )\n",
        "#                     pooled_outputs.append(pooled2)            \n",
        "#=================================================================================#\n",
        "#         for i, filter_size in enumerate(filter_sizes):\n",
        "#           with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "            \n",
        "#             # Convolution Layer\n",
        "#             conv = tf.layers.conv2d(cnn_x,\n",
        "#                                     filters=num_filters,\n",
        "#                                     kernel_size=[filter_size, filter_size],\n",
        "#                                     strides=[1, 1],\n",
        "#                                     activation=tf.nn.relu,)\n",
        "\n",
        "#             # Maxpooling Layer\n",
        "#             pooled = tf.layers.max_pooling2d(conv,\n",
        "#                                     [3, 1],\n",
        "#                                     strides=[2, 1],)\n",
        "#             pooled = tf.transpose(pooled, [0, 1, 3, 2])\n",
        "\n",
        "#             for j, sec_filter_size in enumerate(sec_filter_sizes):\n",
        "#                 with tf.name_scope(\"conv2-maxpool-%s\" % sec_filter_size):\n",
        "#                     # Convolution Layer\n",
        "#                     conv2 = tf.layers.conv2d(pooled,\n",
        "#                                             filters=num_filters,\n",
        "#                                             kernel_size=[sec_filter_size, pooled.shape[2]],\n",
        "#                                             strides=[1, 1],\n",
        "#                                             activation=tf.nn.relu, )\n",
        "#                     print('sec_filter_size: ', sec_filter_size)\n",
        "#                     print('pooled_shape: ', pooled.shape[2])\n",
        "#                     # Maxpooling Layer\n",
        "#                     pooled2 = tf.layers.max_pooling2d(conv2,\n",
        "#                                                      [3, 1],\n",
        "#                                                      strides=[1, 1], )\n",
        "#                     pooled_outputs.append(pooled2) \n",
        "        # Combine all the pooled features\n",
        "        num_filters_total = num_filters * len(filter_sizes)\n",
        "        print('num_filters_total : ', num_filters_total)\n",
        "        h_pool = tf.concat(pooled_outputs, 1)\n",
        "        h_pool_flat = tf.reshape(h_pool, [-1, h_pool.shape[1]*h_pool.shape[2]*h_pool.shape[3]])\n",
        "        print(\"h_pool shape: \", h_pool.shape)\n",
        "\n",
        "        # Add dropout\n",
        "        with tf.name_scope(\"dropout\"):\n",
        "            h_drop = tf.nn.dropout(h_pool_flat, self.output_keep_prob)\n",
        "\n",
        "        fc_1 = tf.contrib.layers.fully_connected(h_drop, int(hidden_layer_size),\n",
        "                                                 activation_fn=tf.nn.relu,)\n",
        "        fc_1 = tf.nn.dropout(fc_1, self.output_keep_prob)                                         \n",
        "        output = tf.contrib.layers.fully_connected(fc_1, 1, activation_fn=None)                    \n",
        "        self.pred = output            \n",
        "#         # Regression layer\n",
        "#         print(\"Output Layer input: \", h_drop.shape)\n",
        "        \n",
        "#         with tf.name_scope(\"Output-Layer\"):\n",
        "#             W = tf.Variable(tf.random_normal([hidden_layer_size, 1], stddev=0.01), name=\"Output_W\")\n",
        "#             b = tf.Variable(tf.random_normal([1], stddev=0.01), name=\"Output_b\")\n",
        "\n",
        "#             output = tf.squeeze(tf.matmul(output, W) + b, 1)\n",
        "#             self.pred = output\n",
        "#         print(\"Output Layer output: \", output.shape)\n",
        "        \n",
        "        \n",
        "        with tf.name_scope(\"Loss\"):\n",
        "            squared_error = tf.square(self.y - output)\n",
        "            self.loss = tf.reduce_mean(squared_error)\n",
        "\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
        "        self.train_step = optimizer.minimize(self.loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQlAiqPOWyov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel:\n",
        "    def __init__(self, args):\n",
        "        # Input Placeholders\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "        \n",
        "        # Model Network....        \n",
        "        \n",
        "        self.pred = None\n",
        "        \n",
        "        # Define loss        \n",
        "        self.loss = None\n",
        "        \n",
        "        # train_step = \"optimize operation\"\n",
        "        self.train_step = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao7Mm93tWyox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_NAME = 'MY_MODEL'\n",
        "\n",
        "if MODEL_NAME == 'LSTM':\n",
        "    model = LSTM(32, 32, False, [], MAXLEN, CHARSIZE, 0.01)\n",
        "    input_x = model.x\n",
        "    output_y = model.y\n",
        "    input_len = model.x_len\n",
        "    pred = model.pred\n",
        "    loss = model.loss\n",
        "    train_op = model.train_step\n",
        "elif MODEL_NAME == 'CNN':\n",
        "    model = Char_CNN(32, [[32, 3, -1], [32, 3, 3]], [], MAXLEN, CHARSIZE, 0.01)\n",
        "    input_x = model.x\n",
        "    output_y = model.y\n",
        "    pred = model.pred\n",
        "    loss = model.loss\n",
        "    train_op = model.train_step\n",
        "elif MODEL_NAME == 'MY_MODEL':\n",
        "    model = MyModel2(32, [[16, 3, -1], [16, 3, 3]], [], MAXLEN, CHARSIZE, 0.0001)\n",
        "    input_x = model.x\n",
        "    output_y = model.y\n",
        "    pred = model.pred\n",
        "    loss = model.loss\n",
        "    train_op = model.train_step\n",
        "    output_keep_prob = model.output_keep_prob\n",
        "else:\n",
        "    raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg0vt77DWyoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 학습 시작하기\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGrOVHazWyo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_rmse = 1000000\n",
        "best_epoch = -1\n",
        "\n",
        "EPOCHs = 100\n",
        "DISPLAY_STEP = 50\n",
        "\n",
        "# TRAINING LOOP\n",
        "for epoch in range(1, EPOCHs + 1):\n",
        "    epoch_loss = 0.0\n",
        "    epoch_start = time.time()\n",
        "    # TRAIN EACH BATCH\n",
        "    for i, (train_x, train_y) in enumerate(train_dataset):\n",
        "        # PAD BATCH DATA\n",
        "        train_x_pad = np.ones((len(train_x), MAXLEN), dtype=np.int32) * 2  # PAD : 2\n",
        "        for idx, s in enumerate(train_x):\n",
        "            length = len(s)\n",
        "            if length < MAXLEN:\n",
        "                train_x_pad[idx, :length] = np.array(s)\n",
        "            else:\n",
        "                train_x_pad[idx:, :MAXLEN] = np.array(s)[:MAXLEN]  # Truncate from the front\n",
        "        \n",
        "        # DEFINE feed_dict\n",
        "        # ex) feed_dict = {input_x: train_x_pad, output_y: train_y}\n",
        "        # feed_dict = {}\n",
        "        feed_dict = {input_x: train_x_pad, output_y: train_y, output_keep_prob:0.3}\n",
        "        \n",
        "        t = time.time()\n",
        "        # train_op: train, optimize\n",
        "        # loss : batch loss\n",
        "        _, l = sess.run([train_op, loss], feed_dict=feed_dict)\n",
        "        elapsed = time.time() - t\n",
        "\n",
        "        epoch_loss += l\n",
        "        \n",
        "        # PRINT LOSS\n",
        "        if (i + 1) % DISPLAY_STEP == 0:\n",
        "            print('[%3d/%3d] loss = %.4f, time elapsed = %.2f' % (i + 1, train_dataset.num_batch, l, elapsed))\n",
        "    epoch_end = time.time() - epoch_start\n",
        "    print('Epoch %3d >> Epoch loss: %.4f , time elapsed %.4f' % (epoch, epoch_loss, epoch_end))\n",
        "    \n",
        "    # EVALUATE ON TEST DATA \n",
        "    se = 0\n",
        "    total = 0\n",
        "    for i, (test_x, test_y) in enumerate(test_dataset):\n",
        "        # PAD\n",
        "        test_x_pad = np.ones((len(test_x), MAXLEN), dtype=np.int32) * 2  # PAD : 2\n",
        "        for idx, s in enumerate(test_x):\n",
        "            length = len(s)\n",
        "            if length < MAXLEN:\n",
        "                test_x_pad[idx, :length] = np.array(s)\n",
        "            else:\n",
        "                test_x_pad[idx:, :MAXLEN] = np.array(s)[:MAXLEN]  # Truncate from the front\n",
        "            \n",
        "        # feed_dict \n",
        "        # feed_dict = {}\n",
        "        feed_dict = {input_x: test_x_pad,  output_keep_prob:1.0}\n",
        "        p = sess.run(pred, feed_dict=feed_dict)\n",
        "        \n",
        "        # squared error\n",
        "        se += np.sum(np.square(p - test_y))\n",
        "        total += len(p)\n",
        "    \n",
        "    # root mean squared error\n",
        "    test_rmse = np.sqrt(se / total)\n",
        "    print('Test RMSE = %.4f' % (test_rmse))\n",
        "    \n",
        "    if best_rmse > test_rmse:\n",
        "        best_rmse = test_rmse\n",
        "        best_epoch = epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnstWfUHWyo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Best RMSE is %.4f at EPOCH %d' % (best_rmse, best_epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HFc7tSXFJdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUpOHYD3-UBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMkv4U2G_c59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}