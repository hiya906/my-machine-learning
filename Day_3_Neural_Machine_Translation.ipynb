{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day_3_Neural_Machine_Translation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiya906/my-machine-learning/blob/master/Day_3_Neural_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD6JGYPfQcnF",
        "colab_type": "text"
      },
      "source": [
        "# **사용법**\n",
        "\n",
        "1.   우측 상단 '로그인'\n",
        "2.   좌측 상단 '실습 모드에서 열기'\n",
        "\n",
        "\n",
        "※ 각각의 셀은 셀 좌측 상단 실행 버튼을 통해 실행할 수 있습니다.\n",
        "\n",
        "※ 실행 중 '경고: 이 노트는 Google에서 작성하지 않았습니다.'라는 창이 뜰 경우, '실행 전에 모든 런타임 재설정'란에 체크 후 '무시하고 계속하기'를 하시면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Neural Machine Translation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CiwtNgENbx2g"
      },
      "source": [
        "## Package import\n",
        "\n",
        "필요한 package 들을 import 시켜줍니다. \n",
        "\n",
        "Colab에서 제공하는 tensorflow는 버전이 1.14 입니다.\n",
        "따라서, tensorflow package를 import 시키기 전에 tensorflow 2.0 버전을 pip install을 사용하여 설치해줍니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnxXKDjq3jEL",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install tensorflow-gpu==2.0.0-beta1\n",
        "import tensorflow as tf\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## Prepare the dataset\n",
        "\n",
        "https://github.com/jungyeul/korean-parallel-corpora 에서 제공하는 학습 데이트를 사용합니다.\n",
        "아래 명령어로 data 폴더를 만든 후, 폴더 안에 train dataset 과 test 데이터 셋을 끌어 놓습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDkAf0fXYjcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 이미 data 폴더가 생성되어 있다면 이 부분을 주석처리 해주세요\n",
        "os.mkdir('./data')\n",
        "\n",
        "train_path = 'data/train'\n",
        "test_path = 'data/test' \n",
        "\n",
        "src = 'ko'\n",
        "tgt = 'en'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_o2V_phUa1p",
        "colab_type": "text"
      },
      "source": [
        "### 1. 문장 내의 특수 문자, 혹은 변환할 수 없는 문자들을 제거하고, 문장의 양 끝에 각각 문장의 시작과 끝을 나타내는 토큰 삽입합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rd0jw-eC3jEh",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s, l):\n",
        "    return ''.join(c for c in unicodedata.normalize(l, s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def normalize(s, l='en'):\n",
        "    cls = ''\n",
        "    if l=='en':\n",
        "        cls = 'NFD'\n",
        "    elif l=='ko':\n",
        "        cls = 'NFKC'\n",
        "    s = unicode_to_ascii(s.lower().strip(), cls)\n",
        "    s = re.sub(r'[\" \"]+', \" \", s)\n",
        "    if l == 'en':\n",
        "        s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "        s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def preprocess_sentence(w, l):\n",
        "    w = normalize(w,l)\n",
        "    w = w.rstrip().strip()\n",
        "    if l == 'en':\n",
        "        w = '^ ' + w + ' $'\n",
        "    elif l == 'ko':\n",
        "        w = '^' + w + '$'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "opI2GzOt479E",
        "colab": {}
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "ko_sentence = u\"제가 이 책을 빌려도 될까요?\"\n",
        "print(preprocess_sentence(en_sentence, 'en').encode('utf-8'))\n",
        "print(preprocess_sentence(ko_sentence, 'ko').encode('utf-8'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAzjADK8UneS",
        "colab_type": "text"
      },
      "source": [
        "### 2. 같은 뜻을 가진 Source와 Target 문장이 각각의 dataset 내에서 같은 순서에 위치하도록 pairing 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OHn4Dct23jEm",
        "colab": {}
      },
      "source": [
        "def create_dataset(path, src, tgt, num_examples):\n",
        "    src_lines = io.open( path + '.' + src, encoding='UTF-8').read().strip().split('\\n')\n",
        "    tgt_lines = io.open( path + '.' + tgt, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[preprocess_sentence(s, src), preprocess_sentence(t, tgt)]  for s, t in zip(src_lines[:num_examples], tgt_lines[:num_examples])]\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cTbSbBz55QtF",
        "colab": {}
      },
      "source": [
        "en, ko = create_dataset(train_path, src, tgt, None)\n",
        "print(en[-1])\n",
        "print(ko[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Un1n5_WAlI",
        "colab_type": "text"
      },
      "source": [
        "### 3. 언어의 특성에 맞게 문장을 토큰 단위로 분리하고 각각의 토큰을 고유한 숫자로 표기합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bIOn8RCNDJXG",
        "colab": {}
      },
      "source": [
        "def tokenize(lang, l):\n",
        "    # 영어는 단어 단위, 한글은 음절 단위로 tokenizing 합니다.\n",
        "    tok = False\n",
        "    if l == 'en':\n",
        "        tok = False\n",
        "    elif l == 'ko':\n",
        "        tok = True\n",
        "    # tf.keras.preprocessing.text.Tokenizer 객체를 생성하고 각각의 언어에 fit 시켜줍니다.\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',char_level=tok)\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    return lang_tokenizer\n",
        "  \n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      # tf.keras.preprocessing.text.Tokenizer의 기본 함수를 이용하여 쉽게 단어를 index로 변환할 수 있습니다.\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "  return\n",
        "\n",
        "def build_vocab(tr, te, l):\n",
        "  lang_tokenizer = tokenize(tr.__add__(te), l)\n",
        "  # 문장을 토큰 단위의 list로 변환합니다.\n",
        "  tr_tensor = lang_tokenizer.texts_to_sequences(tr)\n",
        "  te_tensor = lang_tokenizer.texts_to_sequences(te)\n",
        "  # 가장 긴 문장 길이에 맞춰, 문장 뒤쪽으로 padding 해줍니다.\n",
        "  tr_tensor = tf.keras.preprocessing.sequence.pad_sequences(tr_tensor, padding='post')\n",
        "  te_tensor = tf.keras.preprocessing.sequence.pad_sequences(te_tensor, padding='post')\n",
        "  return tr_tensor, te_tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cnxC7q-j3jFD",
        "colab": {}
      },
      "source": [
        "# 데이터 셋을 로드합니다.\n",
        "num_examples = 30000\n",
        "inp_tr_lang, tgt_tr_lang = create_dataset(train_path, src, tgt, num_examples)\n",
        "inp_te_lang, tgt_te_lang = create_dataset(test_path, src, tgt, num_examples)\n",
        "\n",
        "# 데이터를 tensor의 형태로 변환합니다.\n",
        "inp_tr_tensor, inp_te_tensor, inp_lang = build_vocab(inp_tr_lang, inp_te_lang, src)\n",
        "tgt_tr_tensor, tgt_te_tensor, tgt_lang = build_vocab(tgt_tr_lang, tgt_te_lang, tgt)\n",
        "\n",
        "# 최대 문장 길이를 구하는 부분입니다. tensorflow의 내장함수를 사용하여 padding 하였기 때문에 이미 모든 문장의 길이가 최대 문장 길이와 동일합니다.\n",
        "max_length_tgt = len(tgt_te_tensor[0]) if len(tgt_tr_tensor[0]) < len(tgt_te_tensor[0]) else len(tgt_tr_tensor[0])\n",
        "max_length_inp = len(inp_te_tensor[0]) if len(inp_tr_tensor[0]) < len(inp_te_tensor[0]) else len(inp_tr_lang[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4QILQkOs3jFG",
        "colab": {}
      },
      "source": [
        "# 데이터 각각의 개수를 확인합니다.\n",
        "print(len(inp_tr_tensor), len(tgt_tr_tensor), len(inp_te_tensor), len(tgt_te_tensor))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VXukARTDd7MT",
        "colab": {}
      },
      "source": [
        "print(\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, inp_tr_tensor[0])\n",
        "print()\n",
        "print(\"Target Language; index to word mapping\")\n",
        "convert(tgt_lang, tgt_tr_tensor[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "## Create tf.data.Dataset\n",
        "\n",
        "전처리한 데이터를 이용하여 Dataset 객체를 생성합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TqHsArVZ3jFS",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(inp_tr_tensor)\n",
        "BATCH_SIZE = 32\n",
        "steps_per_epoch = len(inp_tr_tensor)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(tgt_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inp_tr_tensor, tgt_tr_tensor)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qc6-NK1GtWQt",
        "colab": {}
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TNfHIF71ulLu"
      },
      "source": [
        "## Write the encoder model\n",
        "\n",
        "\n",
        "![대체 텍스트](https://postfiles.pstatic.net/MjAxOTA3MjFfOTAg/MDAxNTYzNzEwMzMxODM3.vCTeOFxSzl1rgXoEb9YQ5DAAQbowhEE8WEj1AEEMwDIg.8NHvtAOIivO7r15ko3jt7trDRocpSp2vCHYRv-XICPcg.PNG.er2357/Encoder.png?type=w773)\n",
        "\n",
        "위와 같은 모델 구조를 가진 Encoder를 정의합니다. 정의한 Encoder 모델을 생성하고 호출하여 반환되는 hidden state를 context vector로 사용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nZ2rI24i3jFg",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    \n",
        "    # Embedding layer를 정의합니다. \n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    \n",
        "    # RNN layer를 정의합니다.\n",
        "    self.rnn =  tf.keras.layers.GRU(self.enc_units, return_sequences= True, return_state = True)\n",
        "    \n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    # embedding 함수를 호출하여 x를 embedding 합니다.\n",
        "    x = self.embedding(x)\n",
        "    \n",
        "    # hidden을 초기 state로 하고 embedding 된 문장을 rnn에 넣어줍니다.\n",
        "    output, state = self.rnn(x)\n",
        "    \n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60gSVh05Jl6l",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOZKqs6sau78",
        "colab_type": "text"
      },
      "source": [
        "## Write the Decoder Model\n",
        "\n",
        "![대체 텍스트](https://postfiles.pstatic.net/MjAxOTA3MjFfMTgg/MDAxNTYzNzEwNDI4ODYw.MJxm0wmIKe661tz6Jf_kOjA9T9Cs0QxC91bhGNk3tUQg.WseYOuD4fM-nvNDBn7i6p_NWrqw_jDmvK06yicqtMIwg.PNG.er2357/Decoder.png?type=w773)\n",
        "\n",
        "위와 같은 모델 구조를 가진 Decoder를 정의합니다. 정의한 Decoder 모델을 생성하고 호출할 때 반환되는 값들은 vocabulary 내의 단어들 각각이 현재 step 에서 반환될 확률을 의미합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ_B3mhW3jFk",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    \n",
        "    # Embedding layer를 정의합니다.\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) \n",
        "    \n",
        "    # RNN layer를 정의합니다.\n",
        "    self.rnn =  tf.keras.layers.GRU(self.dec_units, return_sequences= True, return_state = True) \n",
        "    \n",
        "    # Fully Connected Layer를 정의합니다.\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    # embedding 함수를 호출하여 x를 embedding 합니다.\n",
        "    x = self.embedding(x)\n",
        "    \n",
        "    # context vector와 input을 합쳐줍니다.\n",
        "    x = tf.concat([tf.expand_dims(hidden, 1), x], axis=-1)\n",
        "\n",
        "    # RNN layer에 x를 입력합니다.\n",
        "    output, state =  self.rnn(x)\n",
        "    \n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    \n",
        "    # RNN의 output을 FCN layer에 입력합니다.\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P5UY8wko3jFp",
        "colab": {}
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "sample_decoder_output, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmTHr5iV3jFr",
        "colab": {}
      },
      "source": [
        "# Adam optimizer로 옵티마이저를 설정해줍니다.\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# SparseCategoricalCrossentropy loss로 손실함수를 설정합니다.\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  \n",
        "  # 손실 함수를 호출합니다.\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zj8bXQTgNwrF",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hpObfY22IddU"
      },
      "source": [
        "## Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sC9ArXSsVfqn",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # encoder를 호출하여 context vector를 얻습니다. initial state는 전달받은 enc_hidden으로 설정합니다.\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    # context vector를 decoder의 초기 hidden state로 설정합니다.\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    # decoder의 첫번째 input으로 시작토큰인 '^'를 모든 batch에 대해 설정해줍니다.\n",
        "    dec_input = tf.expand_dims([tgt_lang.word_index['^']] * BATCH_SIZE, 1)\n",
        "\n",
        "    \n",
        "    for t in range(1, targ.shape[1]):\n",
        "      \n",
        "      # decoder를 호출하여 현재 step에서 등장할 단어를 예측합니다. initial state는 dec_hidden으로 설정합니다. \n",
        "      predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  # tape 변수를 사용하여 gradient를 계산합니다.\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  # optimizer로 계산한 gradient를 적용합니다.\n",
        "  # \n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ddefjBMa3jF0",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  \n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    \n",
        "    # train_step을 호출하여 batch 당 loss 를 구합니다.\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    \n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Translate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EbQpyYs13jF_",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence, l):\n",
        "    attention_plot = np.zeros((max_length_tgt, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence, l)\n",
        "    \n",
        "    inputs = [i[0] for i in inp_lang.texts_to_sequences(sentence)]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([tgt_lang.word_index['^']], 0)\n",
        "\n",
        "    for t in range(max_length_tgt):\n",
        "        predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += tgt_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if tgt_lang.index_word[predicted_id] == '$':\n",
        "            return result, sentence\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sl9zUHzg3jGI",
        "colab": {}
      },
      "source": [
        "def translate(sentence, l):\n",
        "    result, sentence = evaluate(sentence,l)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UJpT9D5_OgP6",
        "colab": {}
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WrAM0FDomq3E",
        "colab": {}
      },
      "source": [
        "translate(u'도쿄의 니케이 지수는 9291.03으로 0.33 퍼센트 하락했다.', 'ko')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zSx2iM36EZQZ",
        "colab": {}
      },
      "source": [
        "translate(u'기이한 현상이다”고 덧붙였다.', 'ko')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}